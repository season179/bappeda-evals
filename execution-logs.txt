
================================================================================
TESTSET GENERATION - LARGE DOCUMENT MODE
================================================================================

Configuration:
  Documents path: knowledge-files/
  Testset size: 5
  LLM model: openai/gpt-5.1-codex-mini
  Embedding model: qwen/qwen3-embedding-8b

Question Distribution:
  Single-hop: 50% (~2 questions)
    → Questions answerable from ONE document chunk
  Multi-hop (specific): 25% (~1 questions)
    → Fact-based questions requiring MULTIPLE chunks
  Multi-hop (abstract): 25% (~1 questions)
    → Interpretive questions synthesizing across chunks

Chunking Strategy:
  Min tokens per chunk: 300
  Max tokens per chunk: 1000
  → Optimized for large government documents

================================================================================
LOADING DOCUMENTS
================================================================================
  0%|          | 0/8 [00:00<?, ?it/s]100%|██████████| 8/8 [00:00<00:00, 846.29it/s]
/Users/season/Personal/bappeda/rag_eval_ragas/generate_testset_improved.py:331: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))
  generator_llm = LangchainLLMWrapper(openrouter_llm)
/Users/season/Personal/bappeda/rag_eval_ragas/generate_testset_improved.py:337: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings
  generator_embeddings = LangchainEmbeddingsWrapper(

✓ Loaded 8 documents:
  - PERDA NO.3 TAHUN 2025.md: 23,872 characters
  - RPJPD 2025-2045.md: 716,713 characters
  - RPJMD 2025-2029.md: 1,482,952 characters
  - LKPJ 2024.md: 8,828,706 characters
  - PERDA NO.2 TAHUN 2025.md: 13,292 characters
  - RKPD 2025.md: 1,498,643 characters
  - basis_pengetahuan_bappeda_dki_jakarta.md: 14,372 characters
  - PERDA NO.1 TAHUN 2025.md: 16,045 characters

================================================================================
INITIALIZING MODELS
================================================================================
✓ LLM initialized: openai/gpt-5.1-codex-mini (via OpenRouter)
✓ Embeddings initialized: qwen/qwen3-embedding-8b (via OpenRouter)

================================================================================
BUILDING KNOWLEDGE GRAPH
================================================================================

[1/5] Creating initial knowledge graph from documents...
✓ Created 8 document nodes

[2/5] Extracting headlines (max 30 per document)...
  → This identifies sections like 'BAB I', 'BAB II', etc.

[3/5] Splitting documents into chunks (300-1000 tokens)...
  → This breaks large documents into manageable sections

[4/5] Extracting keyphrases (max 15 per chunk)...
  → This identifies key concepts for connecting related chunks

[5/5] Building relationships between chunks...
  → This connects related sections (enables multi-hop questions)

Applying transformations to knowledge graph...
Applying HeadlinesExtractor:   0%|          | 0/8 [00:00<?, ?it/s]Applying HeadlinesExtractor:  12%|█▎        | 1/8 [00:09<01:04,  9.16s/it]Applying HeadlinesExtractor:  25%|██▌       | 2/8 [00:10<00:28,  4.68s/it]Applying HeadlinesExtractor:  38%|███▊      | 3/8 [00:12<00:16,  3.26s/it]Applying HeadlinesExtractor:  50%|█████     | 4/8 [00:15<00:13,  3.40s/it]Applying HeadlinesExtractor:  62%|██████▎   | 5/8 [05:09<05:24, 108.22s/it]